# LASSO 回归优化算法性能对比实验报告

## 王明宽 赵许晴

2025 年 12 月 20 日

---

## 目录

1. [实验概述](#实验概述)

   1. [研究背景](#研究背景)
   2. [研究目标](#研究目标)
2. [理论框架与模型设定](#理论框架与模型设定)

   1. [LASSO 回归模型](#lasso-回归模型)
   2. [优化问题的数学性质](#优化问题的数学性质)
   3. [主要优化算法原理](#主要优化算法原理)
   4. [算法复杂度与收敛性分析](#算法复杂度与收敛性分析)
3. [实验设计](#实验设计)

   1. [数据生成机制与实验场景](#数据生成机制与实验场景)
   2. [评价指标与比较标准](#评价指标与比较标准)
4. [实验结果与分析](#实验结果与分析)

   1. [计算效率核心结论](#计算效率核心结论)
   2. [横向对比_各算法在不同场景的表现](#横向对比_各算法在不同场景的表现)
   3. [纵向对比_固定场景下各算法表现](#纵向对比_固定场景下各算法表现)
5. [结论与方法论讨论](#结论与方法论讨论)

   1. [主要结论](#主要结论)
   2. [LASSO算法选型建议](#LASSO算法选型建议)
6. [附录](#附录)

   1. [核心符号与记号说明](#核心符号与记号说明)
   2. [实验可重复性说明](#实验可重复性说明)

---

# 实验概述

## 研究背景

LASSO回归采用 $$\ell_1$$ 正则化的最小二乘框架，是处理高维数据的经典正则化方法，兼具参数估计和变量选择功能。

其标准形式可表述为如下凸优化问题：

$$
\min_{\beta \in \mathbb{R}^p}
 \frac{1}{2n} |X\beta - y|_2^2 + \lambda |\beta|_1,
$$

其中 $$X \in \mathbb{R}^{n \times p}$$ 为设计矩阵， $$y \in \mathbb{R}^n$$ 为响应变量， $$\beta \in \mathbb{R}^p$$ 为回归系数向量， $$\lambda > 0$$ 为正则化参数。

由于 $$\ell_1$$ 范数在零点处不可微，LASSO 的数值求解本质上是一个**非光滑凸优化问题**。不同优化算法在计算复杂度、收敛速度与数值稳定性方面存在显著差异，这为算法选择与工程实现带来了重要问题。


## 研究目标

本实验旨在对多种经典 LASSO 求解算法进行系统比较，具体目标包括：

1. 比较 ADMM、块坐标下降（BCD）、随机梯度下降（SGD）、次梯度法、FISTA 以及 Huber 损失近似点梯度法在计算效率、收敛行为与估计精度方面的差异；
2. 分析各算法在低维到高维、样本规模变化条件下的数值表现与扩展性；
3. 从算法复杂度与凸优化理论角度解释实验结果；
4. 给出 LASSO 优化算法选型建议。

---

# 理论框架与模型设定

## LASSO 回归模型

考虑线性模型：

$$
y = X\beta + \varepsilon,
$$

其中误差项 $$\varepsilon$$ 满足 $$\mathbb{E}(\varepsilon)=0$$ 。LASSO 估计通过求解以下优化问题得到：

$$
min_{\beta \in \mathbb{R}^p}
\frac{1}{2n} \sum_{i=1}^n (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^p |\beta_j|.
$$

该问题是凸函数(当 $$\lambda > 0$$ 时为非严格凸)，当 $X$ 列满秩且 $$\lambda = 0$$ 时退化为严格凸的最小二乘问题。

---

## 优化问题的数学性质

LASSO 优化问题具有以下关键数学特征：

1. **凸性**：目标函数为凸函数，保证全局最优解的存在性；
2. **非光滑性**: $$|\beta_j|$$ 在 $$\beta_j=0$$ 处不可微，需要借助次梯度或近端算子；
3. **可分性**：正则项在坐标层面可分，为坐标下降类算法提供理论基础。

这些性质直接决定了不同算法在收敛速度与实现复杂度上的差异。


## 主要优化算法原理

### 块坐标下降法（BCD）

在固定其余坐标的条件下，对单一坐标 $$\beta_j$$ 求解一维子问题，可得到闭式更新：

$$
\beta_j^{(k+1)} =
S_{\lambda / n}
(\beta_j^{(k)} - \frac{X_j^\top (X\beta^{(k)} - y)}{|X_j|_2^2})
$$

其中 $$S_\tau(\cdot)$$ 为软阈值算子。由于每一步均精确最小化子问题，该方法在实践中表现出极快的线性收敛速度。


### FISTA（快速迭代收缩阈值算法）

FISTA 属于加速近端梯度法，其核心迭代形式为：

$$
\beta^{(k)} = S_{\lambda/L}
(\theta^{(k)} - \frac{1}{L} \nabla f(\theta^{(k)}))
$$

并通过 Nesterov 动量构造 $$\theta^{(k)}$$ 。理论上，FISTA 在光滑 + 非光滑复合凸问题中可达到 $O(1/k^2)$ 的最优一阶收敛率。

### ADMM（交替方向乘子法）

ADMM 通过变量分裂将原问题转化为多个子问题，并采用原始变量与对偶变量的交替更新进行求解。 ADMM 迭代形式为：

$$
\beta^{(k+1)} = argmin_\beta
\frac{1}{2n}\lVert y - X\beta \rVert_2^2 + \frac{\rho}{2}\lVert \beta - z^{(k)} + u^{(k)} \rVert_2^2
$$

$$
z^{(k+1)} = S_{\lambda/\rho}\left( \beta^{(k+1)} + u^{(k)} \right)
$$

$$
u^{(k+1)} = u^{(k)} + \beta^{(k+1)} - z^{(k+1)}
$$

该方法在约束优化与分布式计算中具有优势，但在高维情形下往往涉及线性系统求解，计算代价较高。


### 随机梯度下降法（Stochastic Gradient Descent, SGD）

随机梯度下降法通过使用单个样本或小批量样本对真实梯度进行近似，从而降低单次迭代的计算成本。其基本更新形式为：

$$
\beta^{(k+1)} = \beta^{(k)} - \eta_k \nabla f_i(\beta^{(k)}),
$$

其中 $$f_i(\cdot)$$ 表示基于随机样本 $i$ 的损失函数, $$\eta_k$$ 为学习率。

在包含 $$\ell_1$$ 正则项的情形下，SGD 往往与近端算子结合，形成近端随机梯度方法：

$$
\beta^{(k+1)} =
S_{\lambda \eta_k}
(\beta^{(k)} - \eta_k \nabla f_i(\beta^{(k)}))
$$

由于梯度噪声的存在，SGD 的理论收敛速度通常为 $$O(1/\sqrt{k})$$ 。


### 次梯度法（Subgradient Method）

次梯度法是梯度下降在不可导凸函数上的自然推广。其迭代更新形式为：

$$
\beta^{(k+1)} = \beta^{(k)} - \eta_k g^{(k)},
$$

其中 $$g^{(k)} \in \partial F(\beta^{(k)})$$ 为目标函数在当前点的一个次梯度。

由于次梯度并非最陡下降方向，该方法的收敛速度较慢，理论上仅能保证函数值以 $$O(1/\sqrt{k})$$ 的速度收敛。


### Huber 近似法

Huber 近似法通过对不可导函数进行平滑近似，使问题可以采用标准梯度型方法求解。以 $$\ell_1$$ 正则项为例，其 Huber 平滑形式可定义为：

$$
\phi_\mu(t) =
\begin{cases}
\frac{t^2}{2\mu}, & |t| \le \mu, \\
|t| - \frac{\mu}{2}, & |t| > \mu,
\end{cases}
$$

从而将正则项近似为：

$$
\lVert \beta \rVert_1 \approx \sum_{j=1}^p \phi_\mu(\beta_j).
$$

经过平滑处理后，目标函数变为光滑函数，可直接使用梯度下降或加速梯度方法进行优化，但需在平滑误差与计算稳定性之间进行权衡。


## 算法复杂度与收敛性分析

| 算法   | 时间复杂度                    | 理论收敛率            |
| ---- | ------------------------ | ---------------- |
| BCD  | $O(np_{\text{active}})$  | 线性收敛             |
| FISTA| $O(np)$                  | $O(1/k^2)$       |
| SGD  | $O(p)$                   | $O(1/\sqrt{k})$  |
| 次梯度 | $O(np)$                  | $O(1/\sqrt{k})$  |
| ADMM | $O(p^3)$                 | $O(1/k)$         |


---

# 实验设计

## 数据生成机制与实验场景

实验采用合成数据，控制样本规模 $$n$$ 、特征维度 $$p$$ 与真实参数稀疏度，构造五类典型统计场景，用以考察算法的稳定性与扩展性。

| 编号 | 样本数（n） | 特征数（p）|稀疏比 | 噪声水平 |  数据分布 | 用途 |
| ----- | ------- | ---- | ------- | ----- | -------- | ------- |
| 1 | 100 | 20  | 0.3 | 0.1 | 正态分布 | 小样本低维场景 |
| 2 | 500  | 50 |  0.3 | 0.1 | 正态分布 | 中样本中维场景 |
| 3 | 1000 | 100 | 0.3 | 0.1 | 正态分布 | 大样本中维场景 |
| 4 | 1000 | 500 | 0.3 | 0.1 | 正态分布 | 大样本高维场景 |
| 5 | 5000 | 1000 | 0.3 | 0.1 | 正态分布 | 超大样本超高维场景 |


## 评价指标与比较标准

* **计算效率**：总运行时间；
* **收敛行为**：达到统一收敛阈值所需迭代次数；
* **估计精度**：均方误差（MSE）：

$$
\mathrm{MSE} = \frac{1}{p} |\hat{\beta} - \beta^*|_2^2.
$$

其中 $\beta^*$ 为数据生成过程中使用的真实回归系数向量（已知）， $\hat{\beta}$ 为算法估计的系数向量。

---

# 实验结果与分析

## 计算效率核心结论

1. 时间效率：BCD 算法在所有 (n,p) 组合下均为最快（耗时 0.0014~0.1462 秒），平均耗时仅 0.0529 秒，是时间效率最优的算法；ADMM 算法耗时最长（平均 11.4860 秒），高维场景下效率劣势显著。
2. 收敛速度：BCD 迭代次数最少（隐含在耗时中），FISTA/SGD 在高维场景下迭代效率更优（单次迭代耗时低）；Subgradient 迭代次数多但总耗时仍可控。
3. 精度效率权衡：FISTA 精度最高（平均 MSE=1.0017e-10），在中样本高维（n=1000,p=500）场景下综合最优；ProximalHuber 在小 / 中样本低 / 中维场景实现 “精度 + 速度” 均衡。


## 横向对比_各算法在不同场景的表现

| 场景 | 场景类型 | 最快算法 | 最精确算法 | 综合最优算法 | 最快算法耗时(秒) | 最精确算法MSE |
|------|----------|----------|------------|--------------|------------------|---------------|
| n=100, p=20 | 小样本低维 | BCD | ProximalHuber | ProximalHuber | 0.0014 | 1.0009434593e-10 |
| n=500, p=50 | 小样本低维 | BCD | ProximalHuber | ProximalHuber | 0.0037 | 1.0000845805e-10 |
| n=1000, p=100 | 中样本中维 | BCD | ProximalHuber | ProximalHuber | 0.0071 | 1.0000423780e-10 |
| n=1000, p=500 | 中样本高维 | BCD | FISTA | FISTA | 0.1061 | 1.0001192477e-10 |
| n=5000, p=1000 | 大样本超高维 | BCD | FISTA | SGD | 0.1462 | 1.0000305291e-10 |

![](https://raw.githubusercontent.com/zhaoxuqing/lasso-/main/横向对比_各算法在不同场景的表现.png)


### BCD

BCD 算法在所有实验设置中均展现出极强的时间优势，其平均训练时间显著低于其他算法。然而，其估计精度整体处于中等水平，未在特定场景下形成明显的精度优势，更适合作为追求快速收敛的基准算法。

### FISTA

FISTA 在精度方面表现最为突出，平均系数 MSE 达到 \(1.00 * 10^{-10}\)。在中样本高维场景下，该算法在时间与精度之间取得了最佳平衡，是综合性能最优的方法之一。

### ADMM

ADMM 的主要优势在于解的稳定性和精度表现，但其计算代价较高，平均训练时间显著高于其他算法。在高维问题中，这一劣势尤为明显，因此更适合对精度要求较高、但对计算时间要求不严格的情形。

### SGD

SGD 在大样本超高维场景中表现突出，凭借较低的单次迭代计算成本实现了较高的整体效率。然而，其估计精度相对有限，更适合作为大规模问题中的快速近似解法。

### 次梯度法

次梯度法具有实现简单、计算稳定的特点，其整体耗时处于中等水平。尽管其理论收敛速度较慢，但在本实验中仍能获得较为可控的训练时间，适合作为对比和基准方法。

### Proximal Huber 近似法

Proximal Huber 方法在小样本低维和中样本中维场景下表现出良好的综合性能，兼顾计算效率与估计精度。其稳健性优势使其在存在噪声或异常值的数据环境中具有较强的实用价值。

## 纵向对比_固定场景下各算法表现

| 算法名称 | 平均训练时间(秒) | 平均系数MSE | 最优适用场景 | 最优场景类 | 核心优势 |
|----------|------------------|-------------|--------------|------------|----------|
| ADMM | 11.486 | 1.8477425900e-03 | 无 | 无 | 精度高 |
| BCD | 0.0529 | 1.8477439174e-03 | 无 | 无 | 速度快 |
| SGD | 0.264 | 3.3575541301e-05 | n=5000, p=1000 | 大样本超高维 | 速度快 |
| Subgradient | 0.6005 | 1.3874830497e-08 | 无 | 无 | 速度快 |
| FISTA | 0.3038 | 1.0017445388e-10 | n=1000, p=500 | 中样本高维 | 速度快 |
| ProximalHuber | 0.3575 | 1.4048948336e-07 | n=100, p=20<br>n=500, p=50<br>n=1000, p=100 | 小样本低维<br>中样本中维 | 速度快 |

![](https://raw.githubusercontent.com/zhaoxuqing/lasso-/main/纵向对比_固定场景下各算法表现.png)


### 小样本低维

所有算法耗时均极低（<0.01 秒），ProximalHuber 综合最优（精度接近 FISTA，速度接近 BCD）

### 中样本中维

趋势与小样本一致，ProximalHuber 仍为综合最优，BCD 仅快但精度略低

### 中样本高维

算法耗时开始分化（BCD=0.1061 秒），FISTA 精度优势凸显，成为综合最优

### 大样本超高维

算法耗时进一步上升（BCD=0.1462 秒），SGD 凭借 “迭代快 + 精度可接受” 成为综合最优


---

# 结论与方法论讨论

## 主要结论

1. 块坐标下降法在 LASSO 问题中兼具理论合理性与工程最优性；
2. FISTA 在缺乏坐标可分结构时提供了接近最优的一阶方法选择；
3. ADMM 与次梯度法在该问题上更多体现教学与理论意义。


## LASSO算法选型建议

### 追求计算速度的场景

推荐算法：**块坐标下降法**（BCD）

理由：BCD充分利用了LASSO问题的坐标可分性，迭代速度快，收敛稳定，尤其是在特征维度较高时仍能保持较低的计算时间。

### 追求估计精度的场景

推荐算法：**FISTA**

理由：FISTA在实验中达到了最高的估计精度，且收敛速度较快。适用于对参数估计精度要求较高的统计推断问题。

### 大规模高维数据场景

推荐算法：**随机梯度下降法**（**SGD**）或**BCD**

理由：SGD单次迭代成本低，适合处理大规模数据；BCD在特征维度高时仍高效。若数据量极大，SGD更合适；若特征维度高但样本量适中，BCD更优。

### 数据存在噪声或异常值的场景

推荐算法：**Huber近似法**

理由：Huber损失对异常值不敏感，同时光滑化后优化过程更稳定，能在保证一定精度的同时提高鲁棒性。

### 综合平衡场景

推荐算法：**FISTA** 或 **Huber近似法**

理由：这两种算法在时间和精度上取得了较好的平衡，适用于大多数实际问题，特别是当对计算时间和精度都有一定要求时。


---

# 附录

## 核心符号与记号说明

| 符号          | 含义          |
| ----------- | ----------- |
| $\beta$     | 回归系数向量      |
| $X$         | 设计矩阵        |
| $y$         | 响应变量        |
| $\lambda$   | 正则化参数       |
| $n$          | 样本数          |
| $p$          | 特征数          |
| $S_\tau(\cdot)$  |  软阈值算子  |


## 实验可重复性说明

实验中固定随机种子，统一数据生成流程与停止准则，确保所有结果可被完全复现。
