# LASSO 回归优化算法性能对比实验报告

## 王明宽 赵许晴

2025 年 12 月 14 日

---

## 目录

1. [实验概述](#实验概述)

   1. [研究背景](#研究背景)
   2. [研究目标](#研究目标)
   3. [实验环境与实现说明](#实验环境与实现说明)
2. [理论框架与模型设定](#理论框架与模型设定)

   1. [LASSO 回归模型](#lasso-回归模型)
   2. [优化问题的数学性质](#优化问题的数学性质)
   3. [主要优化算法原理](#主要优化算法原理)
   4. [算法复杂度与收敛性分析](#算法复杂度与收敛性分析)
3. [实验设计](#实验设计)

   1. [数据生成机制与实验场景](#数据生成机制与实验场景)
   2. [评价指标与比较标准](#评价指标与比较标准)
4. [实验结果与分析](#实验结果与分析)

   1. [计算效率核心结论](#计算效率核心结论)
   2. [横向对比：不同 (n,p) 组合的算法表现差异](#横向对比：不同(n,p)合的算法表现差异)
   3. [纵向对比：不同算法在各个（n,p）组合的表现差异](#纵向对比：不同算法在各个(n,p)组合的表现差异)
5. [结论与方法论讨论](#结论与方法论讨论)

   1. [主要结论](#主要结论)
6. [附录](#附录)

   1. [核心符号与记号说明](#核心符号与记号说明)
   2. [实验可重复性说明](#实验可重复性说明)

---

# 实验概述

## 研究背景

LASSO（Least Absolute Shrinkage and Selection Operator）回归是高维统计与机器学习中最具代表性的正则化方法之一。通过在最小二乘目标函数中引入 $$\ell_1$$ 正则项，LASSO 在参数估计的同时实现变量选择，在高维、小样本问题中具有重要理论与实践价值。

其标准形式可表述为如下凸优化问题：

$$
\min_{\beta \in \mathbb{R}^p}
 \frac{1}{2n} |X\beta - y|_2^2 + \lambda |\beta|_1,
$$

其中 $$X \in \mathbb{R}^{n \times p}$$ 为设计矩阵， $$y \in \mathbb{R}^n$$ 为响应变量， $$\beta \in \mathbb{R}^p$$ 为回归系数向量， $$\lambda > 0$$ 为正则化参数。

由于 $$\ell_1$$ 范数在零点处不可微，LASSO 的数值求解本质上是一个**非光滑凸优化问题**。不同优化算法在计算复杂度、收敛速度与数值稳定性方面存在显著差异，这为算法选择与工程实现带来了重要问题。


## 研究目标

本实验旨在对多种经典 LASSO 求解算法进行系统比较，具体目标包括：

1. 比较 ADMM、块坐标下降（BCD）、随机梯度下降（SGD）、次梯度法、FISTA 以及 Huber 损失近似点梯度法在计算效率、收敛行为与估计精度方面的差异；
2. 分析各算法在低维到高维、样本规模变化条件下的数值表现与扩展性；
3. 从算法复杂度与凸优化理论角度解释实验结果；
4. 给出 LASSO 优化算法选型建议。

## 实验环境与实现说明

* **硬件环境**：Intel Core i7-12700H CPU，32GB 内存
* **软件环境**：Python 3.9，NumPy 1.24，Scikit-learn 1.2，Matplotlib 3.7
* **操作系统**：Windows 11（64 位）
* **实现说明**：所有算法均基于统一的数据生成机制与收敛判据实现，避免实现细节差异带来的偏误。

---

# 理论框架与模型设定

## LASSO 回归模型

考虑线性模型：

$$
y = X\beta + \varepsilon,
$$

其中误差项 $$\varepsilon$$ 满足 $$\mathbb{E}(\varepsilon)=0$$ 。LASSO 估计通过求解以下优化问题得到：

$$
min_{\beta \in \mathbb{R}^p}
\frac{1}{2n} \sum_{i=1}^n (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^p |\beta_j|.
$$

该问题是**严格凸（当 $X$ 满秩）但非光滑**的优化问题，其解在统计意义上具有稀疏性与一致性等良好性质。

---

## 优化问题的数学性质

LASSO 优化问题具有以下关键数学特征：

1. **凸性**：目标函数为凸函数，保证全局最优解的存在性；
2. **非光滑性**: $$|\beta_j|$$ 在 $$\beta_j=0$$ 处不可微，需要借助次梯度或近端算子；
3. **可分性**：正则项在坐标层面可分，为坐标下降类算法提供理论基础。

这些性质直接决定了不同算法在收敛速度与实现复杂度上的差异。


## 主要优化算法原理

### 块坐标下降法（BCD）

在固定其余坐标的条件下，对单一坐标 $$\beta_j$$ 求解一维子问题，可得到闭式更新：

$$
\beta_j^{(k+1)} =
S_{\lambda / n}
\beta_j^{(k)} - \frac{X_j^\top (X\beta^{(k)} - y)}{|X_j|_2^2},
$$

其中 $$S_\tau(\cdot)$$ 为软阈值算子。由于每一步均精确最小化子问题，该方法在实践中表现出极快的线性收敛速度。


### FISTA（快速迭代收缩阈值算法）

FISTA 属于加速近端梯度法，其核心迭代形式为：

$$
\beta^{(k)} = S_{\lambda/L}
\theta^{(k)} - \frac{1}{L} \nabla f(\theta^{(k)}),
$$

并通过 Nesterov 动量构造 $$\theta^{(k)}$$ 。理论上，FISTA 在光滑 + 非光滑复合凸问题中可达到 $O(1/k^2)$ 的最优一阶收敛率。

### ADMM（交替方向乘子法）

ADMM 通过变量分裂将原问题转化为多个子问题，并采用原始变量与对偶变量的交替更新进行求解。 ADMM 迭代形式为：

$$
\beta^{(k+1)} = argmin_\beta
\frac{1}{2n}\lVert y - X\beta \rVert_2^2 + \frac{\rho}{2}\lVert \beta - z^{(k)} + u^{(k)} \rVert_2^2
$$

$$
z^{(k+1)} = S_{\lambda/\rho}\left( \beta^{(k+1)} + u^{(k)} \right)
$$

$$
u^{(k+1)} = u^{(k)} + \beta^{(k+1)} - z^{(k+1)}
$$

该方法在约束优化与分布式计算中具有优势，但在高维情形下往往涉及线性系统求解，计算代价较高。


### 随机梯度下降法（Stochastic Gradient Descent, SGD）

随机梯度下降法通过使用单个样本或小批量样本对真实梯度进行近似，从而降低单次迭代的计算成本。其基本更新形式为：

$$
\beta^{(k+1)} = \beta^{(k)} - \eta_k \nabla f_i(\beta^{(k)}),
$$

其中 $$f_i(\cdot)$$ 表示基于随机样本 $i$ 的损失函数, $$\eta_k$$ 为学习率。

在包含 $$\ell_1$$ 正则项的情形下，SGD 往往与近端算子结合，形成近端随机梯度方法：

$$
\beta^{(k+1)} =
S_{\lambda \eta_k}
\left(
\beta^{(k)} - \eta_k \nabla f_i(\beta^{(k)})
\right).
$$

由于梯度噪声的存在，SGD 的理论收敛速度通常为 $$O(1/\sqrt{k})$$ 。


### 次梯度法（Subgradient Method）

次梯度法是梯度下降在不可导凸函数上的自然推广。其迭代更新形式为：

$$
\beta^{(k+1)} = \beta^{(k)} - \eta_k g^{(k)},
$$

其中 $$g^{(k)} \in \partial F(\beta^{(k)})$$ 为目标函数在当前点的一个次梯度。

由于次梯度并非最陡下降方向，该方法的收敛速度较慢，理论上仅能保证函数值以 $$O(1/\sqrt{k})$$ 的速度收敛。


### Huber 近似法

Huber 近似法通过对不可导函数进行平滑近似，使问题可以采用标准梯度型方法求解。以 $$\ell_1$$ 正则项为例，其 Huber 平滑形式可定义为：

$$
\phi_\mu(t) =
\begin{cases}
\frac{t^2}{2\mu}, & |t| \le \mu, \\
|t| - \frac{\mu}{2}, & |t| > \mu,
\end{cases}
$$

从而将正则项近似为：

$$
\lVert \beta \rVert_1 \approx \sum_{j=1}^p \phi_\mu(\beta_j).
$$

经过平滑处理后，目标函数变为光滑函数，可直接使用梯度下降或加速梯度方法进行优化，但需在平滑误差与计算稳定性之间进行权衡。


## 算法复杂度与收敛性分析

| 算法   | 时间复杂度                    | 理论收敛率            |
| ---- | ------------------------ | ---------------- |
| BCD  | $O(np_{\text{active}})$  | 线性收敛             |
| FISTA| $O(np)$                  | $O(1/k^2)$       |
| SGD  | $O(p)$                   | $O(1/\sqrt{k})$  |
| 次梯度 | $O(np)$                  | $O(1/\sqrt{k})$  |
| ADMM | $O(p^3)$                 | $O(1/k)$         |


---

# 实验设计

## 数据生成机制与实验场景

实验采用合成数据，控制样本规模 $$n$$ 、特征维度 $$p$$ 与真实参数稀疏度，构造五类典型高维统计场景，用以考察算法的稳定性与扩展性。


## 评价指标与比较标准

* **计算效率**：总运行时间；
* **收敛行为**：达到统一收敛阈值所需迭代次数；
* **估计精度**：均方误差（MSE）：

$$
\mathrm{MSE} = \frac{1}{p} |\hat{\beta} - \beta^*|_2^2.
$$


---

# 实验结果与分析

## 计算效率核心结论
1. 时间效率：BCD 算法在所有 (n,p) 组合下均为最快（耗时 0.0014~0.1462 秒），平均耗时仅 0.0529 秒，是时间效率最优的算法；ADMM 算法耗时最长（平均 11.4860 秒），高维场景下效率劣势显著。
2. 收敛速度：BCD 迭代次数最少（隐含在耗时中），FISTA/SGD 在高维场景下迭代效率更优（单次迭代耗时低）；Subgradient 迭代次数多但总耗时仍可控。
3. 精度效率权衡：FISTA 精度最高（平均 MSE=1.0017e-10），在中样本高维（n=1000,p=500）场景下综合最优；ProximalHuber 在小 / 中样本低 / 中维场景实现 “精度 + 速度” 均衡。


## 横向对比：不同 (n,p) 组合的算法表现差异

![](https://raw.githubusercontent.com/zhaoxuqing/lasso-/main/横向对比_各算法在不同场景的表现.png)


### 小样本低维

所有算法耗时均极低（<0.01 秒），ProximalHuber 综合最优（精度接近 FISTA，速度接近 BCD）

### 中样本中维

趋势与小样本一致，ProximalHuber 仍为综合最优，BCD 仅快但精度略低

### 中样本高维

算法耗时开始分化（BCD=0.1061 秒），FISTA 精度优势凸显，成为综合最优

### 大样本超高维

算法耗时进一步上升（BCD=0.1462 秒），SGD 凭借 “迭代快 + 精度可接受” 成为综合最优

## 纵向对比：不同算法在各个（n,p）组合的表现差异

![](https://raw.githubusercontent.com/zhaoxuqing/lasso-/main/纵向对比_固定场景下各算法表现.png)

### BCD

BCD 算法在所有实验设置中均展现出极强的时间优势，其平均训练时间显著低于其他算法。然而，其估计精度整体处于中等水平，未在特定场景下形成明显的精度优势，更适合作为追求快速收敛的基准算法。

### FISTA

FISTA 在精度方面表现最为突出，平均系数 MSE 达到 \(1.00 \times 10^{-10}\)。在中样本高维场景下，该算法在时间与精度之间取得了最佳平衡，是综合性能最优的方法之一。

### ADMM

ADMM 的主要优势在于解的稳定性和精度表现，但其计算代价较高，平均训练时间显著高于其他算法。在高维问题中，这一劣势尤为明显，因此更适合对精度要求较高、但对计算时间要求不严格的情形。

### SGD

SGD 在大样本超高维场景中表现突出，凭借较低的单次迭代计算成本实现了较高的整体效率。然而，其估计精度相对有限，更适合作为大规模问题中的快速近似解法。

### 次梯度法

次梯度法具有实现简单、计算稳定的特点，其整体耗时处于中等水平。尽管其理论收敛速度较慢，但在本实验中仍能获得较为可控的训练时间，适合作为对比和基准方法。

### Proximal Huber 近似法

Proximal Huber 方法在小样本低维和中样本中维场景下表现出良好的综合性能，兼顾计算效率与估计精度。其稳健性优势使其在存在噪声或异常值的数据环境中具有较强的实用价值。

---

# 结论与方法论讨论

## 主要结论

1. 块坐标下降法在 LASSO 问题中兼具理论合理性与工程最优性；
2. FISTA 在缺乏坐标可分结构时提供了接近最优的一阶方法选择；
3. ADMM 与次梯度法在该问题上更多体现教学与理论意义。


---

# 附录

## 核心符号与记号说明

| 符号          | 含义          |
| ----------- | ----------- |
| $\beta$     | 回归系数向量      |
| $X$         | 设计矩阵        |
| $y$         | 响应变量        |
| $\lambda$   | 正则化参数       |


## 实验可重复性说明

实验中固定随机种子，统一数据生成流程与停止准则，确保所有结果可被完全复现。
